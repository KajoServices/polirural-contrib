{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SVM model from CrisiLex data - inf_source classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect_langs\n",
    "from langdetect import detect\n",
    "import sys\n",
    "from spacy.matcher import Matcher\n",
    "# scikit-learn version 0.22.2.post1\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod sm vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisLexT26/2012_Venezuela_refinery/2012_Venezuela_refinery-tweets_labeled.csv\n",
      "all\n",
      "CrisisLexT26/2013_Australia_bushfire/2013_Australia_bushfire-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_NY_train_crash/2013_NY_train_crash-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Brazil_nightclub_fire/2013_Brazil_nightclub_fire-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Alberta_floods/2013_Alberta_floods-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Philipinnes_floods/2012_Philipinnes_floods-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Sardinia_floods/2013_Sardinia_floods-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Colorado_wildfires/2012_Colorado_wildfires-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Singapore_haze/2013_Singapore_haze-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Colorado_floods/2013_Colorado_floods-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_LA_airport_shootings/2013_LA_airport_shootings-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Lac_Megantic_train_crash/2013_Lac_Megantic_train_crash-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Russia_meteor/2013_Russia_meteor-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_West_Texas_explosion/2013_West_Texas_explosion-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Guatemala_earthquake/2012_Guatemala_earthquake-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Glasgow_helicopter_crash/2013_Glasgow_helicopter_crash-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Costa_Rica_earthquake/2012_Costa_Rica_earthquake-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Italy_earthquakes/2012_Italy_earthquakes-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Queensland_floods/2013_Queensland_floods-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Boston_bombings/2013_Boston_bombings-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Typhoon_Yolanda/2013_Typhoon_Yolanda-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2012_Typhoon_Pablo/2012_Typhoon_Pablo-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Savar_building_collapse/2013_Savar_building_collapse-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Bohol_earthquake/2013_Bohol_earthquake-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Spain_train_crash/2013_Spain_train_crash-tweets_labeled.csv\n",
      "new\n",
      "CrisisLexT26/2013_Manila_floods/2013_Manila_floods-tweets_labeled.csv\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "# read all CrisiLex data from subdirectories\n",
    "# data from https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26\n",
    "df_all = pd.DataFrame(columns = ['tweet_id', 'tweet_text', 'inf_source', 'inf_type', 'informativeness'])\n",
    "for dirpath, dirnames, filenames in os.walk(\"CrisisLexT26\"):\n",
    "    for filename in [f for f in filenames if f.endswith(\"labeled.csv\")]:\n",
    "        print (os.path.join(dirpath, filename))\n",
    "        if df_all.empty:\n",
    "            print('all')\n",
    "            df_all = pd.read_csv(os.path.join(dirpath, filename), names = ['tweet_id', 'tweet_text', 'inf_source', 'inf_type', 'informativeness'])\n",
    "        else:\n",
    "            print('new')\n",
    "            df_new = pd.read_csv(os.path.join(dirpath, filename), names = ['tweet_id', 'tweet_text', 'inf_source', 'inf_type', 'informativeness'])\n",
    "            df_all = pd.concat([df_all, df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect language from text\n",
    "def tweet_parser_lang(tweet):\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    #https://issue.life/questions/43388476\n",
    "    # hasthags in single token (normally the #sigh is standalone token)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "    matches = matcher(doc)\n",
    "    hashtags = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        hashtags.append(doc[start:end])\n",
    "    for span in hashtags:\n",
    "        span.merge()\n",
    "\n",
    "    new_list=[]\n",
    "    for token in doc:\n",
    "        if token.text == 'RT': continue\n",
    "        #remove hashtags\n",
    "        if token.text.startswith('#'): \n",
    "            continue\n",
    "        if token.text.startswith('@'):\n",
    "            continue\n",
    "        if token.pos_ == 'NUM': \n",
    "            continue  \n",
    "        if token.like_url: \n",
    "            continue\n",
    "        if token.is_space: continue\n",
    "\n",
    "        new_list.append(token.text.lower())\n",
    "    return (' '.join(new_list))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets (0.003551 seconds per tweet)\n",
      "Processed 100 tweets (0.029720 seconds per tweet)\n",
      "Processed 200 tweets (0.028398 seconds per tweet)\n",
      "Processed 300 tweets (0.027279 seconds per tweet)\n",
      "Processed 400 tweets (0.028633 seconds per tweet)\n",
      "Processed 500 tweets (0.027825 seconds per tweet)\n",
      "Processed 600 tweets (0.026489 seconds per tweet)\n",
      "Processed 700 tweets (0.027673 seconds per tweet)\n",
      "Processed 800 tweets (0.027263 seconds per tweet)\n",
      "Processed 900 tweets (0.029477 seconds per tweet)\n",
      "Processed 1000 tweets (0.029726 seconds per tweet)\n",
      "Processed 1100 tweets (0.029560 seconds per tweet)\n",
      "Processed 1200 tweets (0.034884 seconds per tweet)\n",
      "Processed 1300 tweets (0.031951 seconds per tweet)\n",
      "Processed 1400 tweets (0.029006 seconds per tweet)\n",
      "Processed 1500 tweets (0.031652 seconds per tweet)\n",
      "Processed 1600 tweets (0.027667 seconds per tweet)\n",
      "Processed 1700 tweets (0.030799 seconds per tweet)\n",
      "Processed 1800 tweets (0.029175 seconds per tweet)\n",
      "Processed 1900 tweets (0.037395 seconds per tweet)\n",
      "Processed 2000 tweets (0.031968 seconds per tweet)\n",
      "Processed 2100 tweets (0.032725 seconds per tweet)\n",
      "Processed 2200 tweets (0.036498 seconds per tweet)\n",
      "Processed 2300 tweets (0.018384 seconds per tweet)\n",
      "Processed 2400 tweets (0.020234 seconds per tweet)\n",
      "Processed 2500 tweets (0.020015 seconds per tweet)\n",
      "Processed 2600 tweets (0.019670 seconds per tweet)\n",
      "Processed 2700 tweets (0.020310 seconds per tweet)\n",
      "Processed 2800 tweets (0.020236 seconds per tweet)\n",
      "Processed 2900 tweets (0.018948 seconds per tweet)\n",
      "Processed 3000 tweets (0.018772 seconds per tweet)\n",
      "Processed 3100 tweets (0.019385 seconds per tweet)\n",
      "Processed 3200 tweets (0.021305 seconds per tweet)\n",
      "Processed 3300 tweets (0.023822 seconds per tweet)\n",
      "Processed 3400 tweets (0.030565 seconds per tweet)\n",
      "Processed 3500 tweets (0.029712 seconds per tweet)\n",
      "Processed 3600 tweets (0.026926 seconds per tweet)\n",
      "Processed 3700 tweets (0.025633 seconds per tweet)\n",
      "Processed 3800 tweets (0.019007 seconds per tweet)\n",
      "Processed 3900 tweets (0.024341 seconds per tweet)\n",
      "Processed 4000 tweets (0.025738 seconds per tweet)\n",
      "Processed 4100 tweets (0.022118 seconds per tweet)\n",
      "Processed 4200 tweets (0.019885 seconds per tweet)\n",
      "Processed 4300 tweets (0.035912 seconds per tweet)\n",
      "Processed 4400 tweets (0.031621 seconds per tweet)\n",
      "Processed 4500 tweets (0.034884 seconds per tweet)\n",
      "Processed 4600 tweets (0.034877 seconds per tweet)\n",
      "Processed 4700 tweets (0.033597 seconds per tweet)\n",
      "Processed 4800 tweets (0.034634 seconds per tweet)\n",
      "Processed 4900 tweets (0.037317 seconds per tweet)\n",
      "Processed 5000 tweets (0.032994 seconds per tweet)\n",
      "Processed 5100 tweets (0.035574 seconds per tweet)\n",
      "Processed 5200 tweets (0.032957 seconds per tweet)\n",
      "Processed 5300 tweets (0.026687 seconds per tweet)\n",
      "Processed 5400 tweets (0.030351 seconds per tweet)\n",
      "Processed 5500 tweets (0.028654 seconds per tweet)\n",
      "Processed 5600 tweets (0.034584 seconds per tweet)\n",
      "Processed 5700 tweets (0.033703 seconds per tweet)\n",
      "Processed 5800 tweets (0.034167 seconds per tweet)\n",
      "Processed 5900 tweets (0.032896 seconds per tweet)\n",
      "Processed 6000 tweets (0.030192 seconds per tweet)\n",
      "Processed 6100 tweets (0.029552 seconds per tweet)\n",
      "Processed 6200 tweets (0.031079 seconds per tweet)\n",
      "Processed 6300 tweets (0.036755 seconds per tweet)\n",
      "Processed 6400 tweets (0.038020 seconds per tweet)\n",
      "Processed 6500 tweets (0.034916 seconds per tweet)\n",
      "Processed 6600 tweets (0.033500 seconds per tweet)\n",
      "Processed 6700 tweets (0.034073 seconds per tweet)\n",
      "Processed 6800 tweets (0.036653 seconds per tweet)\n",
      "Processed 6900 tweets (0.040106 seconds per tweet)\n",
      "Processed 7000 tweets (0.035923 seconds per tweet)\n",
      "Processed 7100 tweets (0.040627 seconds per tweet)\n",
      "Processed 7200 tweets (0.040583 seconds per tweet)\n",
      "Processed 7300 tweets (0.035178 seconds per tweet)\n",
      "Processed 7400 tweets (0.026738 seconds per tweet)\n",
      "Processed 7500 tweets (0.027254 seconds per tweet)\n",
      "Processed 7600 tweets (0.029863 seconds per tweet)\n",
      "Processed 7700 tweets (0.027336 seconds per tweet)\n",
      "Processed 7800 tweets (0.023292 seconds per tweet)\n",
      "Processed 7900 tweets (0.022928 seconds per tweet)\n",
      "Processed 8000 tweets (0.024140 seconds per tweet)\n",
      "Processed 8100 tweets (0.021818 seconds per tweet)\n",
      "Processed 8200 tweets (0.027258 seconds per tweet)\n",
      "Processed 8300 tweets (0.023337 seconds per tweet)\n",
      "Processed 8400 tweets (0.030934 seconds per tweet)\n",
      "Processed 8500 tweets (0.026278 seconds per tweet)\n",
      "Processed 8600 tweets (0.024804 seconds per tweet)\n",
      "Processed 8700 tweets (0.028488 seconds per tweet)\n",
      "Processed 8800 tweets (0.028774 seconds per tweet)\n",
      "Processed 8900 tweets (0.028108 seconds per tweet)\n",
      "Processed 9000 tweets (0.029726 seconds per tweet)\n",
      "Processed 9100 tweets (0.029512 seconds per tweet)\n",
      "Processed 9200 tweets (0.029722 seconds per tweet)\n",
      "Processed 9300 tweets (0.029922 seconds per tweet)\n",
      "Processed 9400 tweets (0.030052 seconds per tweet)\n",
      "Processed 9500 tweets (0.034412 seconds per tweet)\n",
      "Processed 9600 tweets (0.035418 seconds per tweet)\n",
      "Processed 9700 tweets (0.028741 seconds per tweet)\n",
      "Processed 9800 tweets (0.031903 seconds per tweet)\n",
      "Processed 9900 tweets (0.025932 seconds per tweet)\n",
      "Processed 10000 tweets (0.029866 seconds per tweet)\n",
      "Processed 10100 tweets (0.029845 seconds per tweet)\n",
      "Processed 10200 tweets (0.027020 seconds per tweet)\n",
      "Processed 10300 tweets (0.027872 seconds per tweet)\n",
      "Processed 10400 tweets (0.030381 seconds per tweet)\n",
      "Processed 10500 tweets (0.029742 seconds per tweet)\n",
      "Processed 10600 tweets (0.025064 seconds per tweet)\n",
      "Processed 10700 tweets (0.026382 seconds per tweet)\n",
      "Processed 10800 tweets (0.026924 seconds per tweet)\n",
      "Processed 10900 tweets (0.027290 seconds per tweet)\n",
      "Processed 11000 tweets (0.025854 seconds per tweet)\n",
      "Processed 11100 tweets (0.026476 seconds per tweet)\n",
      "Processed 11200 tweets (0.028867 seconds per tweet)\n",
      "Processed 11300 tweets (0.024916 seconds per tweet)\n",
      "Processed 11400 tweets (0.027351 seconds per tweet)\n",
      "Processed 11500 tweets (0.038515 seconds per tweet)\n",
      "Processed 11600 tweets (0.024213 seconds per tweet)\n",
      "Processed 11700 tweets (0.024453 seconds per tweet)\n",
      "Processed 11800 tweets (0.026199 seconds per tweet)\n",
      "Processed 11900 tweets (0.022239 seconds per tweet)\n",
      "Processed 12000 tweets (0.023258 seconds per tweet)\n",
      "Processed 12100 tweets (0.022253 seconds per tweet)\n",
      "Processed 12200 tweets (0.021847 seconds per tweet)\n",
      "Processed 12300 tweets (0.024979 seconds per tweet)\n",
      "Processed 12400 tweets (0.023968 seconds per tweet)\n",
      "Processed 12500 tweets (0.022031 seconds per tweet)\n",
      "Processed 12600 tweets (0.023017 seconds per tweet)\n",
      "Processed 12700 tweets (0.024364 seconds per tweet)\n",
      "Processed 12800 tweets (0.026108 seconds per tweet)\n",
      "Processed 12900 tweets (0.024395 seconds per tweet)\n",
      "Processed 13000 tweets (0.023002 seconds per tweet)\n",
      "Processed 13100 tweets (0.022947 seconds per tweet)\n",
      "Processed 13200 tweets (0.025753 seconds per tweet)\n",
      "Processed 13300 tweets (0.022353 seconds per tweet)\n",
      "Processed 13400 tweets (0.024392 seconds per tweet)\n",
      "Processed 13500 tweets (0.027379 seconds per tweet)\n",
      "Processed 13600 tweets (0.023271 seconds per tweet)\n",
      "Processed 13700 tweets (0.019041 seconds per tweet)\n",
      "Processed 13800 tweets (0.020046 seconds per tweet)\n",
      "Processed 13900 tweets (0.021108 seconds per tweet)\n",
      "Processed 14000 tweets (0.023840 seconds per tweet)\n",
      "Processed 14100 tweets (0.024156 seconds per tweet)\n",
      "Processed 14200 tweets (0.025040 seconds per tweet)\n",
      "Processed 14300 tweets (0.025992 seconds per tweet)\n",
      "Processed 14400 tweets (0.026760 seconds per tweet)\n",
      "Processed 14500 tweets (0.025791 seconds per tweet)\n",
      "Processed 14600 tweets (0.026758 seconds per tweet)\n",
      "Processed 14700 tweets (0.027016 seconds per tweet)\n",
      "Processed 14800 tweets (0.024594 seconds per tweet)\n",
      "Processed 14900 tweets (0.030323 seconds per tweet)\n",
      "Processed 15000 tweets (0.028304 seconds per tweet)\n",
      "Processed 15100 tweets (0.028885 seconds per tweet)\n",
      "Processed 15200 tweets (0.026193 seconds per tweet)\n",
      "Processed 15300 tweets (0.024494 seconds per tweet)\n",
      "Processed 15400 tweets (0.022901 seconds per tweet)\n",
      "Processed 15500 tweets (0.021718 seconds per tweet)\n",
      "Processed 15600 tweets (0.023840 seconds per tweet)\n",
      "Processed 15700 tweets (0.028183 seconds per tweet)\n",
      "Processed 15800 tweets (0.027982 seconds per tweet)\n",
      "Processed 15900 tweets (0.033726 seconds per tweet)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16000 tweets (0.027953 seconds per tweet)\n",
      "Processed 16100 tweets (0.019580 seconds per tweet)\n",
      "Processed 16200 tweets (0.024283 seconds per tweet)\n",
      "Processed 16300 tweets (0.021690 seconds per tweet)\n",
      "Processed 16400 tweets (0.022808 seconds per tweet)\n",
      "Processed 16500 tweets (0.022219 seconds per tweet)\n",
      "Processed 16600 tweets (0.024693 seconds per tweet)\n",
      "Processed 16700 tweets (0.022787 seconds per tweet)\n",
      "Processed 16800 tweets (0.022723 seconds per tweet)\n",
      "Processed 16900 tweets (0.026228 seconds per tweet)\n",
      "Processed 17000 tweets (0.024572 seconds per tweet)\n",
      "Processed 17100 tweets (0.029165 seconds per tweet)\n",
      "Processed 17200 tweets (0.027745 seconds per tweet)\n",
      "Processed 17300 tweets (0.027590 seconds per tweet)\n",
      "Processed 17400 tweets (0.025231 seconds per tweet)\n",
      "Processed 17500 tweets (0.027430 seconds per tweet)\n",
      "Processed 17600 tweets (0.027790 seconds per tweet)\n",
      "Processed 17700 tweets (0.028597 seconds per tweet)\n",
      "Processed 17800 tweets (0.027211 seconds per tweet)\n",
      "Processed 17900 tweets (0.026057 seconds per tweet)\n",
      "Processed 18000 tweets (0.032411 seconds per tweet)\n",
      "Processed 18100 tweets (0.043503 seconds per tweet)\n",
      "Processed 18200 tweets (0.037650 seconds per tweet)\n",
      "Processed 18300 tweets (0.043020 seconds per tweet)\n",
      "Processed 18400 tweets (0.040317 seconds per tweet)\n",
      "Processed 18500 tweets (0.038153 seconds per tweet)\n",
      "Processed 18600 tweets (0.030353 seconds per tweet)\n",
      "Processed 18700 tweets (0.028136 seconds per tweet)\n",
      "Processed 18800 tweets (0.028508 seconds per tweet)\n",
      "Processed 18900 tweets (0.030686 seconds per tweet)\n",
      "Processed 19000 tweets (0.032138 seconds per tweet)\n",
      "Processed 19100 tweets (0.031814 seconds per tweet)\n",
      "Processed 19200 tweets (0.027936 seconds per tweet)\n",
      "Processed 19300 tweets (0.031047 seconds per tweet)\n",
      "Processed 19400 tweets (0.032025 seconds per tweet)\n",
      "Processed 19500 tweets (0.031729 seconds per tweet)\n",
      "Processed 19600 tweets (0.033794 seconds per tweet)\n",
      "Processed 19700 tweets (0.033848 seconds per tweet)\n",
      "Processed 19800 tweets (0.030412 seconds per tweet)\n",
      "Processed 19900 tweets (0.028828 seconds per tweet)\n",
      "Processed 20000 tweets (0.030091 seconds per tweet)\n",
      "Processed 20100 tweets (0.027767 seconds per tweet)\n",
      "Processed 20200 tweets (0.029791 seconds per tweet)\n",
      "Processed 20300 tweets (0.033739 seconds per tweet)\n",
      "Processed 20400 tweets (0.036462 seconds per tweet)\n",
      "Processed 20500 tweets (0.026844 seconds per tweet)\n",
      "Processed 20600 tweets (0.031173 seconds per tweet)\n",
      "Processed 20700 tweets (0.026314 seconds per tweet)\n",
      "Processed 20800 tweets (0.023768 seconds per tweet)\n",
      "Processed 20900 tweets (0.025776 seconds per tweet)\n",
      "Processed 21000 tweets (0.029208 seconds per tweet)\n",
      "Processed 21100 tweets (0.024907 seconds per tweet)\n",
      "Processed 21200 tweets (0.026049 seconds per tweet)\n",
      "Processed 21300 tweets (0.022793 seconds per tweet)\n",
      "Processed 21400 tweets (0.024105 seconds per tweet)\n",
      "Processed 21500 tweets (0.022933 seconds per tweet)\n",
      "Processed 21600 tweets (0.018833 seconds per tweet)\n",
      "Processed 21700 tweets (0.025550 seconds per tweet)\n",
      "Processed 21800 tweets (0.030052 seconds per tweet)\n",
      "Processed 21900 tweets (0.028518 seconds per tweet)\n",
      "Processed 22000 tweets (0.029516 seconds per tweet)\n",
      "Processed 22100 tweets (0.028758 seconds per tweet)\n",
      "Processed 22200 tweets (0.033005 seconds per tweet)\n",
      "Processed 22300 tweets (0.029536 seconds per tweet)\n",
      "Processed 22400 tweets (0.030456 seconds per tweet)\n",
      "Processed 22500 tweets (0.030013 seconds per tweet)\n",
      "Processed 22600 tweets (0.033596 seconds per tweet)\n",
      "Processed 22700 tweets (0.033878 seconds per tweet)\n",
      "Processed 22800 tweets (0.029913 seconds per tweet)\n",
      "Processed 22900 tweets (0.030774 seconds per tweet)\n",
      "Processed 23000 tweets (0.031525 seconds per tweet)\n",
      "Processed 23100 tweets (0.028263 seconds per tweet)\n",
      "Processed 23200 tweets (0.028803 seconds per tweet)\n",
      "Processed 23300 tweets (0.029196 seconds per tweet)\n",
      "Processed 23400 tweets (0.026550 seconds per tweet)\n",
      "Processed 23500 tweets (0.027164 seconds per tweet)\n",
      "Processed 23600 tweets (0.028477 seconds per tweet)\n",
      "Processed 23700 tweets (0.027911 seconds per tweet)\n",
      "Processed 23800 tweets (0.026195 seconds per tweet)\n",
      "Processed 23900 tweets (0.023635 seconds per tweet)\n",
      "Processed 24000 tweets (0.026610 seconds per tweet)\n",
      "Processed 24100 tweets (0.026110 seconds per tweet)\n",
      "Processed 24200 tweets (0.025917 seconds per tweet)\n",
      "Processed 24300 tweets (0.029695 seconds per tweet)\n",
      "Processed 24400 tweets (0.033569 seconds per tweet)\n",
      "Processed 24500 tweets (0.032304 seconds per tweet)\n",
      "Processed 24600 tweets (0.024558 seconds per tweet)\n",
      "Processed 24700 tweets (0.025818 seconds per tweet)\n",
      "Processed 24800 tweets (0.032293 seconds per tweet)\n",
      "Processed 24900 tweets (0.041810 seconds per tweet)\n",
      "Processed 25000 tweets (0.034196 seconds per tweet)\n",
      "Processed 25100 tweets (0.027206 seconds per tweet)\n",
      "Processed 25200 tweets (0.030976 seconds per tweet)\n",
      "Processed 25300 tweets (0.032000 seconds per tweet)\n",
      "Processed 25400 tweets (0.031619 seconds per tweet)\n",
      "Processed 25500 tweets (0.030717 seconds per tweet)\n",
      "Processed 25600 tweets (0.030849 seconds per tweet)\n",
      "Processed 25700 tweets (0.029394 seconds per tweet)\n",
      "Processed 25800 tweets (0.032176 seconds per tweet)\n",
      "Processed 25900 tweets (0.030549 seconds per tweet)\n",
      "Processed 26000 tweets (0.029205 seconds per tweet)\n",
      "Processed 26100 tweets (0.020916 seconds per tweet)\n",
      "Processed 26200 tweets (0.021422 seconds per tweet)\n",
      "Processed 26300 tweets (0.020265 seconds per tweet)\n",
      "Processed 26400 tweets (0.023247 seconds per tweet)\n",
      "Processed 26500 tweets (0.021121 seconds per tweet)\n",
      "Processed 26600 tweets (0.019199 seconds per tweet)\n",
      "Processed 26700 tweets (0.019405 seconds per tweet)\n",
      "Processed 26800 tweets (0.020108 seconds per tweet)\n",
      "Processed 26900 tweets (0.020039 seconds per tweet)\n",
      "Processed 27000 tweets (0.024117 seconds per tweet)\n",
      "Processed 27100 tweets (0.033231 seconds per tweet)\n",
      "Processed 27200 tweets (0.031058 seconds per tweet)\n",
      "Processed 27300 tweets (0.039317 seconds per tweet)\n",
      "Processed 27400 tweets (0.036708 seconds per tweet)\n",
      "Processed 27500 tweets (0.038346 seconds per tweet)\n",
      "Processed 27600 tweets (0.036347 seconds per tweet)\n",
      "Processed 27700 tweets (0.037684 seconds per tweet)\n",
      "Processed 27800 tweets (0.033636 seconds per tweet)\n",
      "Processed 27900 tweets (0.037114 seconds per tweet)\n",
      "tweet_id           19335\n",
      "tweet_text         19335\n",
      "inf_source         19335\n",
      "inf_type           19335\n",
      "informativeness    19335\n",
      "language_spacy     19335\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# parse all tweets and detect language\n",
    "\n",
    "tweets = df_all['tweet_text'].values.tolist()\n",
    "if 'language_detector' not in nlp.pipe_names: \n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "        \n",
    "i=0\n",
    "list_lang_spacy=[]\n",
    "counter = 0\n",
    "logging = True\n",
    "start = timer()\n",
    "parsed_tweets=[]\n",
    "all_entities=[]\n",
    "\n",
    "for tweet in tweets:\n",
    "    # parse tweet for language detection\n",
    "    lang_parsed=tweet_parser_lang(tweet)\n",
    "    doc=nlp(lang_parsed)\n",
    "    list_lang_spacy.append(doc._.language[\"language\"])\n",
    "    \n",
    "\n",
    "    if counter % 100 == 0 and logging:\n",
    "        end = timer()\n",
    "        print(\"Processed %d tweets (%f seconds per tweet)\" % (counter, (end-start)/100))\n",
    "        start = timer()\n",
    "    counter += 1  \n",
    "    #if counter > 300: break\n",
    "\n",
    "df_all['language_spacy']=list_lang_spacy \n",
    "\n",
    "#just en tweets\n",
    "en_tweets = df_all[(df_all.language_spacy == 'en')]\n",
    "\n",
    "print(en_tweets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27959 entries, 0 to 1000\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         27959 non-null  object\n",
      " 1   tweet_text       27959 non-null  object\n",
      " 2   inf_source       27959 non-null  object\n",
      " 3   inf_type         27959 non-null  object\n",
      " 4   informativeness  27959 non-null  object\n",
      " 5   language_spacy   27959 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# save \n",
    "df_all.to_pickle(\"crisislex_df_all_lang.pkl\")\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27959 entries, 0 to 1000\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         27959 non-null  object\n",
      " 1   tweet_text       27959 non-null  object\n",
      " 2   inf_source       27959 non-null  object\n",
      " 3   inf_type         27959 non-null  object\n",
      " 4   informativeness  27959 non-null  object\n",
      " 5   language_spacy   27959 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.5+ MB\n",
      "tweet_id           19335\n",
      "tweet_text         19335\n",
      "inf_source         19335\n",
      "inf_type           19335\n",
      "informativeness    19335\n",
      "language_spacy     19335\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  load pickle\n",
    "df_all = pd.read_pickle(\"crisislex_df_all_lang.pkl\")\n",
    "en_tweets = df_all[(df_all.language_spacy == 'en')]\n",
    "df_all.info()\n",
    "\n",
    "#just en tweets\n",
    "en_tweets = df_all[(df_all.language_spacy == 'en')]\n",
    "print(en_tweets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweet text\n",
    "def tweet_parser_lemma(tweet_to_parse):\n",
    "        \"\"\" parse tweet text for classification \"\"\"\n",
    "        tweet = tweet_to_parse\n",
    "\n",
    "        tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
    "        tweet = re.sub(r\"^RT\\s\", \"\", tweet)\n",
    "        tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "        doc = nlp(tweet)\n",
    "        new_list = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'NUM': continue\n",
    "            if token.like_url: continue\n",
    "            if token.is_space: continue\n",
    "            if token.is_punct: continue\n",
    "            if token.is_stop: continue\n",
    "            #small tokens\n",
    "            if len(token.text) < 3: continue\n",
    "            new_list.append(token.lemma_.lower())\n",
    "        return ' '.join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load EN word2vec model\n",
    "model = KeyedVectors.load_word2vec_format('/home/dzon/kajo/new2/semexp-data/data/w2v-jiri-slim-extended.bin', binary=True)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document vector - average of all tokens' w2v vectors\n",
    "def get_doc_vector(text):\n",
    "    #doc=nlp(text)\n",
    "    doc=text.lower().split(\" \")\n",
    "    vector_final = np.zeros(model.vector_size, dtype=np.float32)\n",
    "    tokens_count = 0\n",
    "    for token in doc:\n",
    "        #token = t.text.lower()\n",
    "        if token in model:\n",
    "            vector_final = np.add(model[token], vector_final)\n",
    "            tokens_count += 1\n",
    "        else:\n",
    "            pass\n",
    "            #print(token, \"not found in model\")\n",
    "    if tokens_count:\n",
    "        return vector_final / tokens_count\n",
    "    else:\n",
    "        return vector_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17404 entries, 21 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         17404 non-null  object\n",
      " 1   tweet_text       17404 non-null  object\n",
      " 2   inf_source       17404 non-null  object\n",
      " 3   inf_type         17404 non-null  object\n",
      " 4   informativeness  17404 non-null  object\n",
      " 5   language_spacy   17404 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 951.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# remove not labeled\n",
    "df_en_inf_source = en_tweets[en_tweets['inf_source'] != 'Not labeled']\n",
    "df_en_inf_source.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.08378340099989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzon/kajo/kajo_semexp_core/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# clean texts\n",
    "start = timer()\n",
    "df_en_inf_source['parsed_text_lemma'] = df_en_inf_source.tweet_text.apply(tweet_parser_lemma)\n",
    "end = timer()\n",
    "print(end-start)\n",
    "\n",
    "# 102.41539139300585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13531 entries, 21 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   tweet_id           13531 non-null  object\n",
      " 1   tweet_text         13531 non-null  object\n",
      " 2   inf_source         13531 non-null  object\n",
      " 3   inf_type           13531 non-null  object\n",
      " 4   informativeness    13531 non-null  object\n",
      " 5   language_spacy     13531 non-null  object\n",
      " 6   parsed_text_lemma  13531 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 845.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzon/kajo/kajo_semexp_core/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "df_en_inf_source.drop_duplicates(subset =\"parsed_text_lemma\", \n",
    "                     keep = False, inplace = True) \n",
    "df_en_inf_source.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28853491299923917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzon/kajo/kajo_semexp_core/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# get doc vectors for all tweets\n",
    "start = timer()\n",
    "df_en_inf_source['doc_vector_gensim_lemma'] = df_en_inf_source.parsed_text_lemma.apply(get_doc_vector)\n",
    "end = timer()\n",
    "print(end-start)\n",
    "\n",
    "# 0.27816161201917566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13523 entries, 21 to 999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   tweet_id                 13523 non-null  object\n",
      " 1   tweet_text               13523 non-null  object\n",
      " 2   inf_source               13523 non-null  object\n",
      " 3   inf_type                 13523 non-null  object\n",
      " 4   informativeness          13523 non-null  object\n",
      " 5   language_spacy           13523 non-null  object\n",
      " 6   parsed_text_lemma        13523 non-null  object\n",
      " 7   doc_vector_gensim_lemma  13523 non-null  object\n",
      " 8   doc_vector_gensim_zero   13523 non-null  bool  \n",
      "dtypes: bool(1), object(8)\n",
      "memory usage: 964.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzon/kajo/kajo_semexp_core/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# clean zero vectors\n",
    "def is_np_zero(doc_vector):\n",
    "    if not np.any(doc_vector):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_en_inf_source['doc_vector_gensim_zero'] = df_en_inf_source.doc_vector_gensim_lemma.apply(is_np_zero)\n",
    "\n",
    "df_en_inf_source2 = df_en_inf_source[df_en_inf_source['doc_vector_gensim_zero'] == False]\n",
    "df_en_inf_source2.info()\n",
    "df_en_inf_source = df_en_inf_source2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([154.97156501, 158.12615428, 158.07256265, 161.2266016 ,\n",
       "        137.28413434]),\n",
       " 'std_fit_time': array([ 1.58733124,  0.97356602,  2.16818184,  0.60226565, 47.60175943]),\n",
       " 'mean_score_time': array([26.12726417, 26.189044  , 26.13410296, 25.00040588, 20.17209387]),\n",
       " 'std_score_time': array([0.74810669, 0.76605027, 0.45104795, 0.57603632, 7.15916565]),\n",
       " 'param_C': masked_array(data=[1.8, 2, 2.2, 2.4, 2.6],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1.8}, {'C': 2}, {'C': 2.2}, {'C': 2.4}, {'C': 2.6}],\n",
       " 'split0_test_score': array([0.62338262, 0.62384473, 0.62430684, 0.62615527, 0.62707948]),\n",
       " 'split1_test_score': array([0.62292052, 0.62707948, 0.62892791, 0.62892791, 0.6284658 ]),\n",
       " 'split2_test_score': array([0.62338262, 0.62292052, 0.62476895, 0.62107209, 0.62014787]),\n",
       " 'split3_test_score': array([0.61534905, 0.61488673, 0.61534905, 0.61488673, 0.61673601]),\n",
       " 'split4_test_score': array([0.62552011, 0.62459547, 0.62459547, 0.62367083, 0.62413315]),\n",
       " 'mean_test_score': array([0.62211099, 0.62266539, 0.62358964, 0.62294257, 0.62331246]),\n",
       " 'std_test_score': array([0.00349958, 0.0041273 , 0.00445693, 0.00479732, 0.00435167]),\n",
       " 'rank_test_score': array([5, 4, 1, 3, 2], dtype=int32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find C parameter for SVM\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_en_inf_source['doc_vector_gensim_lemma'].tolist(), df_en_inf_source['inf_source'].tolist(), test_size=0.2, random_state=1)\n",
    "\n",
    "parameters = {'C':[1.8, 2, 2.2, 2.4, 2.6]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=12)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "sorted(clf.cv_results_.keys())\n",
    "clf.cv_results_\n",
    "\n",
    "# best C is 2.2 with 0.8 train 0.62358964\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation accuracy: 0.46964961882991174 [0.56192237 0.36598891 0.40924214 0.47300296 0.53809172]\n",
      "time of execution: 577.4775641279994\n"
     ]
    }
   ],
   "source": [
    "#5-fold cross validation\n",
    "start = timer()\n",
    "clf = svm.SVC(probability = True, C=2.2)\n",
    "#clf = OneVsRestClassifier(svm.SVC(probability = True, C=2.2), n_jobs=6)\n",
    "\n",
    "score = cross_val_score(clf, df_en_inf_source['doc_vector_gensim_lemma'].tolist(), df_en_inf_source['inf_source'].tolist(), cv=5, n_jobs=12)\n",
    "score_mean = score.mean()\n",
    "print('cross validation accuracy: {} {}'.format(score_mean, score))\n",
    "end = timer()\n",
    "print('time of execution: {}'.format(end-start))\n",
    "\n",
    "# svm.SVC()\n",
    "#cross validation accuracy: 0.6030703413964448 [0.62655123 0.63182212 0.61184971 0.58571842 0.55941023]\n",
    "#time of execution: 389.2026017999999\n",
    "\n",
    "# onevsRest\n",
    "#cross validation accuracy: 0.6052061666308802 [0.63290043 0.63904129 0.60578035 0.58514021 0.56316855]\n",
    "#time of execution: 803.8628881\n",
    "\n",
    "# OneVsRestClassifier(svm.SVC) with gensim vectors n_jobs=6\n",
    "# cross validation accuracy: 0.5805079351688035 [0.60691824 0.61835334 0.59348199 0.56591364 0.51787246]\n",
    "# time of execution: 1878.5939883560059\n",
    "\n",
    "#sVC C 1.7, rbf lemma, stop_words left\n",
    "#cross validation accuracy: 0.5058419459039138 [0.58810653 0.42028457 0.46041591 0.49653411 0.56386861]\n",
    "#time of execution: 507.3958388370229\n",
    "\n",
    "# lemma stop_words left OneVsRestClassifier(svm.SVC(probability = True, C=1.7), n_jobs=6)\n",
    "#cross validation accuracy: 0.4950426212395177 [0.58591755 0.41627143 0.43232397 0.48157607 0.55912409]\n",
    "#time of execution: 1305.4485383289866\n",
    "\n",
    "# lemma gensim stop words removed clf = svm.SVC(probability = True, C=1.7)\n",
    "#cross validation accuracy: 0.47866421158169636 [0.56619822 0.38054734 0.42455621 0.4818787  0.54014058]\n",
    "\n",
    "# lemma, gensim, clf = OneVsRestClassifier(svm.SVC(probability = True, C=2.2), n_jobs=6)\n",
    "# cross validation accuracy: 0.46395525489724265 [0.55859519 0.37079482 0.39556377 0.45710059 0.53772189]\n",
    "#time of execution: 1371.3395205349952\n",
    "\n",
    "# lemma, gensim clf = svm.SVC(probability = True, C=2.2)\n",
    "#cross validation accuracy: 0.46964961882991174 [0.56192237 0.36598891 0.40924214 0.47300296 0.53809172]\n",
    "#time of execution: 577.4775641279994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.8352744010044\n"
     ]
    }
   ],
   "source": [
    "# train the classificator\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(df_en_inf_source['doc_vector_gensim'].tolist(), df_en_inf_source['inf_source'].tolist(), test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = df_en_inf_source['doc_vector_gensim_lemma'].tolist()\n",
    "y_train = df_en_inf_source['inf_source'].tolist()\n",
    "\n",
    "start = timer()\n",
    "#clf = OneVsRestClassifier(svm.SVC(probability = True, C=2.2), n_jobs=12).fit(X_train, y_train)\n",
    "clf = svm.SVC(probability = True, C=2.2).fit(X_train, y_train)\n",
    "end = timer()\n",
    "print(end-start)\n",
    "\n",
    "# n_jobs = 12 OneVsRest\n",
    "# 454.67108336198726\n",
    "\n",
    "# svm.SVC(probability = True, C=1.7) full nodup - n_jobs not available\n",
    "# 381.09414758501225\n",
    "\n",
    "# svm.SVC(probability = True, C=2.2).fit(X_train, y_train)\n",
    "# 398.8352744010044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481.11828097000034\n"
     ]
    }
   ],
   "source": [
    "# train the classificator\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(df_en_inf_source['doc_vector_gensim'].tolist(), df_en_inf_source['inf_source'].tolist(), test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = df_en_inf_source['doc_vector_gensim_lemma'].tolist()\n",
    "y_train = df_en_inf_source['inf_source'].tolist()\n",
    "\n",
    "start = timer()\n",
    "clf_ovr = OneVsRestClassifier(svm.SVC(probability = True, C=2.2), n_jobs=12).fit(X_train, y_train)\n",
    "\n",
    "end = timer()\n",
    "print(end-start)\n",
    "\n",
    "# OneVsRestClassifier(svm.SVC(probability = True, C=2.2), n_jobs=12)\n",
    "# 479.3168061140168\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "#df_en_inf_source['inf_source_predict_svc_gensim_lemma'] = clf.predict(df_en_inf_source['doc_vector_gensim_lemma'].tolist())\n",
    "df_en_inf_source['inf_source_predict_svc_ovr_gensim_lemma'] = clf_ovr.predict(df_en_inf_source['doc_vector_gensim_lemma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC OVR:\n",
      "accuracy:  0.8077349700510241\n",
      "balanced_accuracy_score:  0.6494065418477704\n",
      "f1 weighted:  0.8021917369598703\n",
      "precision_score:  0.8214334854250069\n",
      "recall_score:  0.8077349700510241\n"
     ]
    }
   ],
   "source": [
    "# examine accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#print(\"SVC:\")\n",
    "#y_true = df_en_inf_source['inf_source']\n",
    "#y_pred = df_en_inf_source['inf_source_predict_svc_gensim_lemma']\n",
    "#print(\"accuracy: \", accuracy_score(y_true, y_pred))\n",
    "#print(\"balanced_accuracy_score: \", balanced_accuracy_score(y_true, y_pred))\n",
    "#print(\"f1 weighted: \",f1_score(y_true, y_pred, average='weighted'))\n",
    "#print(\"precision_score: \",precision_score(y_true, y_pred, average='weighted'))\n",
    "#print(\"recall_score: \",recall_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "print(\"SVC OVR:\")\n",
    "y_true = df_en_inf_source['inf_source']\n",
    "y_pred = df_en_inf_source['inf_source_predict_svc_ovr_gensim_lemma']\n",
    "print(\"accuracy: \", accuracy_score(y_true, y_pred))\n",
    "print(\"balanced_accuracy_score: \", balanced_accuracy_score(y_true, y_pred))\n",
    "print(\"f1 weighted: \",f1_score(y_true, y_pred, average='weighted'))\n",
    "print(\"precision_score: \",precision_score(y_true, y_pred, average='weighted'))\n",
    "print(\"recall_score: \",recall_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "# nodup gensim full\n",
    "# accuracy:  0.7200029212006135\n",
    "# balanced_accuracy_score:  0.4579787673009016\n",
    "# f1 weighted:  0.7000478277689586\n",
    "# precision_score:  0.7424681261869172\n",
    "# recall_score:  0.7200029212006135\n",
    "\n",
    "# cvm.SVC C:1.7\n",
    "#accuracy:  0.7291316731176514\n",
    "#balanced_accuracy_score:  0.43019976596449566\n",
    "#f1 weighted:  0.7046239632759524\n",
    "#precision_score:  0.7467371055740926\n",
    "#recall_score:  0.7291316731176514\n",
    "\n",
    "# svc C 1.7 lemma\n",
    "#accuracy:  0.7414086172492794\n",
    "#balanced_accuracy_score:  0.46293556557232174\n",
    "#f1 weighted:  0.7207575861828118\n",
    "#precision_score:  0.7617823631216263\n",
    "#recall_score:  0.7414086172492794\n",
    "\n",
    "# lemma with left stop words\n",
    "#accuracy:  0.7259924109748979\n",
    "#balanced_accuracy_score:  0.4257827643220438\n",
    "#f1 weighted:  0.7008060844142024\n",
    "#precision_score:  0.743965975009386\n",
    "#recall_score:  0.7259924109748979\n",
    "\n",
    "# C=2.2 SVC lemma, without stopwords, without duplicates, without zero vectors\n",
    "#accuracy:  0.7686164312652518\n",
    "#balanced_accuracy_score:  0.5224920473992108\n",
    "#f1 weighted:  0.7540863378990941\n",
    "#precision_score:  0.7851196477953029\n",
    "#recall_score:  0.7686164312652518\n",
    "\n",
    "# C=2.2 SVC OVR lemma, without stopwords, without duplicates, without zero vectors\n",
    "#accuracy:  0.8089920875545367\n",
    "#balanced_accuracy_score:  0.6620251424688365\n",
    "#f1 weighted:  0.8039184872477424\n",
    "#precision_score:  0.8230064098474439\n",
    "#recall_score:  0.8089920875545367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv to investigate results\n",
    "df_en_inf_source[['tweet_text','parsed_text_lemma','inf_source','inf_source_predict_svc_gensim_lemma', 'inf_source_predict_svc_ovr_gensim_lemma']].to_csv('CrisisLex_all_parsed_en_inf_source_predicted_gensim_SVC_C2.2.csv', sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outsiders accuracy: 0.830343300110742 (correct: 3749 uncorrect: 766)\n",
      "Media accuracy: 0.761189677795597 (correct: 5221 uncorrect: 1638)\n",
      "Not applicable accuracy: 0.9833333333333333 (correct: 59 uncorrect: 1)\n",
      "Government accuracy: 0.9626168224299065 (correct: 309 uncorrect: 12)\n",
      "Eyewitness accuracy: 0.8977900552486188 (correct: 975 uncorrect: 111)\n",
      "Business accuracy: 0.9642857142857143 (correct: 135 uncorrect: 5)\n",
      "NGOs accuracy: 0.8763837638376384 (correct: 475 uncorrect: 67)\n",
      "total accuracy: 0.8077349700510241 (correct: 10923, uncorrect: 2600 total: 13523 13523\n"
     ]
    }
   ],
   "source": [
    "# accuracy for all classses\n",
    "correct_sum=0\n",
    "uncorrect_sum=0\n",
    "for unique in df_en_inf_source.inf_source.unique():\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_onevsrest_gensim==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source!=unique) & (df_en_inf_source.inf_source_predict_onevsrest_gensim==unique)])\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source!=unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma==unique)])\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma_stop==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source!=unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma_stop==unique)])\n",
    "    correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_ovr_gensim_lemma==unique)])\n",
    "    uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source!=unique) & (df_en_inf_source.inf_source_predict_svc_ovr_gensim_lemma==unique)])\n",
    "    \n",
    "    correct_sum+=correct\n",
    "    uncorrect_sum+=uncorrect\n",
    "    print(\"{} accuracy: {} (correct: {} uncorrect: {})\".format(unique,correct/(correct+uncorrect),correct,uncorrect))\n",
    "print(\"total accuracy: {} (correct: {}, uncorrect: {} total: {} {}\".format(correct_sum/(correct_sum+uncorrect_sum), correct_sum, uncorrect_sum, (correct_sum+uncorrect_sum), len(df_en_inf_source.inf_source)))      \n",
    "\n",
    "# svc C:1.7 full nodup nolemma\n",
    "#Outsiders accuracy: 0.735352730171383 (correct: 3690 uncorrect: 1328)\n",
    "#Media accuracy: 0.7155220547180347 (correct: 5126 uncorrect: 2038)\n",
    "#Not applicable accuracy: 1.0 (correct: 15 uncorrect: 0)\n",
    "#Government accuracy: 0.9 (correct: 99 uncorrect: 11)\n",
    "#Eyewitness accuracy: 0.7936333699231614 (correct: 723 uncorrect: 188)\n",
    "#Business accuracy: 1.0 (correct: 29 uncorrect: 0)\n",
    "#NGOs accuracy: 0.6771300448430493 (correct: 302 uncorrect: 144)\n",
    "#total accuracy: 0.7291316731176514 (correct: 9984, uncorrect: 3709 total: 13693 13693\n",
    "\n",
    "# lemma\n",
    "#Outsiders accuracy: 0.7763043478260869 (correct: 3571 uncorrect: 1029)\n",
    "#Media accuracy: 0.706953642384106 (correct: 5124 uncorrect: 2124)\n",
    "#Not applicable accuracy: 1.0 (correct: 20 uncorrect: 0)\n",
    "#Government accuracy: 0.9236641221374046 (correct: 121 uncorrect: 10)\n",
    "#Eyewitness accuracy: 0.8111332007952287 (correct: 816 uncorrect: 190)\n",
    "#Business accuracy: 1.0 (correct: 33 uncorrect: 0)\n",
    "#NGOs accuracy: 0.7038539553752535 (correct: 347 uncorrect: 146)\n",
    "#total accuracy: 0.7414086172492794 (correct: 10032, uncorrect: 3499 total: 13531 13531\n",
    "\n",
    "#lemma with left stop words\n",
    "#Outsiders accuracy: 0.7390606182256122 (correct: 3682 uncorrect: 1300)\n",
    "#Media accuracy: 0.7085643015521065 (correct: 5113 uncorrect: 2103)\n",
    "#Not applicable accuracy: 1.0 (correct: 18 uncorrect: 0)\n",
    "#Government accuracy: 0.8952380952380953 (correct: 94 uncorrect: 11)\n",
    "#Eyewitness accuracy: 0.7822318526543879 (correct: 722 uncorrect: 201)\n",
    "#Business accuracy: 1.0 (correct: 20 uncorrect: 0)\n",
    "#NGOs accuracy: 0.6818181818181818 (correct: 300 uncorrect: 140)\n",
    "#total accuracy: 0.7259924109748979 (correct: 9949, uncorrect: 3755 total: 13704 13704\n",
    "\n",
    "# C=2.2 SVC OVR lemma, without stopwords, without duplicates, without zero vectors without init_sims\n",
    "#Outsiders accuracy: 0.8353099127321548 (correct: 3733 uncorrect: 736)\n",
    "#Media accuracy: 0.7598373747640482 (correct: 5233 uncorrect: 1654)\n",
    "#Not applicable accuracy: 1.0 (correct: 66 uncorrect: 0)\n",
    "#Government accuracy: 0.963076923076923 (correct: 313 uncorrect: 12)\n",
    "#Eyewitness accuracy: 0.9031365313653137 (correct: 979 uncorrect: 105)\n",
    "#Business accuracy: 0.9602649006622517 (correct: 145 uncorrect: 6)\n",
    "#NGOs accuracy: 0.8706099815157117 (correct: 471 uncorrect: 70)\n",
    "#total accuracy: 0.8089920875545367 (correct: 10940, uncorrect: 2583 total: 13523 13523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outsiders accuracy: 0.7887649905322954 (correct: 3749 uncorrect: 1004)\n",
      "Media accuracy: 0.9214613483939287 (correct: 5221 uncorrect: 445)\n",
      "Not applicable accuracy: 0.4013605442176871 (correct: 59 uncorrect: 88)\n",
      "Government accuracy: 0.4327731092436975 (correct: 309 uncorrect: 405)\n",
      "Eyewitness accuracy: 0.7200886262924667 (correct: 975 uncorrect: 379)\n",
      "Business accuracy: 0.5357142857142857 (correct: 135 uncorrect: 117)\n",
      "NGOs accuracy: 0.7456828885400314 (correct: 475 uncorrect: 162)\n"
     ]
    }
   ],
   "source": [
    "# accuracy for all classses - impostors\n",
    "for unique in df_en_inf_source.inf_source.unique():\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_onevsrest_gensim==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_onevsrest_gensim!=unique)])\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma!=unique)])\n",
    "    #correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma_stop==unique)])\n",
    "    #uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_gensim_lemma_stop!=unique)])\n",
    "    correct = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_ovr_gensim_lemma==unique)])\n",
    "    uncorrect = len(df_en_inf_source[(df_en_inf_source.inf_source==unique) & (df_en_inf_source.inf_source_predict_svc_ovr_gensim_lemma!=unique)])\n",
    "    \n",
    "    print(\"{} accuracy: {} (correct: {} uncorrect: {})\".format(unique,correct/(correct+uncorrect),correct,uncorrect))\n",
    "#print(\"total: {}\".format(len(df_en_inf_source.inf_source)))    \n",
    "\n",
    "# svc C:1.7 full nodup nolemma\n",
    "#Outsiders accuracy: 0.7646083713220058 (correct: 3690 uncorrect: 1136)\n",
    "#Media accuracy: 0.8944337811900192 (correct: 5126 uncorrect: 605)\n",
    "#Not applicable accuracy: 0.10135135135135136 (correct: 15 uncorrect: 133)\n",
    "#Government accuracy: 0.13636363636363635 (correct: 99 uncorrect: 627)\n",
    "#Eyewitness accuracy: 0.5300586510263929 (correct: 723 uncorrect: 641)\n",
    "#Business accuracy: 0.11196911196911197 (correct: 29 uncorrect: 230)\n",
    "#NGOs accuracy: 0.4726134585289515 (correct: 302 uncorrect: 337)\n",
    "\n",
    "# lemma without stop words\n",
    "#Outsiders accuracy: 0.751473063973064 (correct: 3571 uncorrect: 1181)\n",
    "#Media accuracy: 0.9030666196686641 (correct: 5124 uncorrect: 550)\n",
    "#Not applicable accuracy: 0.1360544217687075 (correct: 20 uncorrect: 127)\n",
    "#Government accuracy: 0.1678224687933426 (correct: 121 uncorrect: 600)\n",
    "#Eyewitness accuracy: 0.6048925129725723 (correct: 816 uncorrect: 533)\n",
    "#Business accuracy: 0.12992125984251968 (correct: 33 uncorrect: 221)\n",
    "#NGOs accuracy: 0.5473186119873817 (correct: 347 uncorrect: 287)\n",
    "\n",
    "# lemma with stop words\n",
    "#Outsiders accuracy: 0.7635835752799668 (correct: 3682 uncorrect: 1140)\n",
    "#Media accuracy: 0.8903012362876546 (correct: 5113 uncorrect: 630)\n",
    "#Not applicable accuracy: 0.12162162162162163 (correct: 18 uncorrect: 130)\n",
    "#Government accuracy: 0.1289437585733882 (correct: 94 uncorrect: 635)\n",
    "#Eyewitness accuracy: 0.5293255131964809 (correct: 722 uncorrect: 642)\n",
    "#Business accuracy: 0.07722007722007722 (correct: 20 uncorrect: 239)\n",
    "#NGOs accuracy: 0.4694835680751174 (correct: 300 uncorrect: 339)\n",
    "\n",
    "# C=2.2 SVC OVR lemma, without stopwords, without duplicates, without zero vectors without init_sims\n",
    "#Outsiders accuracy: 0.7853986955606985 (correct: 3733 uncorrect: 1020)\n",
    "#Media accuracy: 0.9235792446170138 (correct: 5233 uncorrect: 433)\n",
    "#Not applicable accuracy: 0.4489795918367347 (correct: 66 uncorrect: 81)\n",
    "#Government accuracy: 0.438375350140056 (correct: 313 uncorrect: 401)\n",
    "#Eyewitness accuracy: 0.7230428360413589 (correct: 979 uncorrect: 375)\n",
    "#Business accuracy: 0.5753968253968254 (correct: 145 uncorrect: 107)\n",
    "#NGOs accuracy: 0.7394034536891679 (correct: 471 uncorrect: 166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'CrisisLex-inf_source.model'\n",
    "pickle.dump(clf_ovr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index = 50\n",
    "#timeit 11.3 ms ± 119 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "result = clf.predict(df_en_inf_source['doc_vector_gensim'].tolist()[index:index+1])\n",
    "\n",
    "result_probs = list(clf.predict_proba(df_en_inf_source['doc_vector_gensim'].tolist()[index:index+1])[0])\n",
    "print(\"tweet '{}'(index {}) was classified as: {} (truth: {})\".format(df_en_inf_source['tweet_text'].tolist()[index:index+1][0],index,result[0],df_en_inf_source['inf_source'].tolist()[index:index+1][0]))\n",
    "#df_en_inf_source['tweet_text'][2:3].tolist()\n",
    "#print(type(result_probs))\n",
    "zipped = zip(clf.classes_,result_probs)\n",
    "zipped = sorted(zipped, key=lambda x: x[1],reverse=True)\n",
    "for unique,prob in zipped:\n",
    "    print (\"{} : {:.2f}\".format(unique,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model to disk\n",
    "filename = 'CrisisLex-inf_source.model'\n",
    "clf_ovr = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: Strong quake off Guatemala is felt in Mexico City - http://t.co/fpjpvqES http://t.co/Y23hxGaF\n",
      "Media : 0.67\n",
      "Outsiders : 0.20\n",
      "Eyewitness : 0.08\n",
      "Government : 0.02\n",
      "Not applicable : 0.01\n",
      "Business : 0.01\n",
      "NGOs : 0.01\n",
      "\n",
      "thank you for all your concerns earthquake was scary but thankfully everyone praying for italy\n",
      "Outsiders : 0.76\n",
      "Eyewitness : 0.14\n",
      "Media : 0.06\n",
      "NGOs : 0.01\n",
      "Not applicable : 0.01\n",
      "Government : 0.01\n",
      "Business : 0.01\n",
      "\n",
      "we have water outside\n",
      "Eyewitness : 0.52\n",
      "Media : 0.21\n",
      "Outsiders : 0.13\n",
      "NGOs : 0.10\n",
      "Government : 0.03\n",
      "Not applicable : 0.01\n",
      "Business : 0.01\n",
      "\n",
      "help me! my house is destroyed i don't have roof\n",
      "Eyewitness : 0.63\n",
      "Outsiders : 0.13\n",
      "Media : 0.11\n",
      "Government : 0.06\n",
      "NGOs : 0.04\n",
      "Business : 0.02\n",
      "Not applicable : 0.01\n",
      "\n",
      "newspaper informs about earthquake in the ocean\n",
      "Media : 0.56\n",
      "Outsiders : 0.26\n",
      "Eyewitness : 0.09\n",
      "Business : 0.05\n",
      "NGOs : 0.02\n",
      "Not applicable : 0.02\n",
      "Government : 0.01\n",
      "\n",
      "cannot access my car\n",
      "Media : 0.41\n",
      "Outsiders : 0.22\n",
      "Government : 0.17\n",
      "Business : 0.08\n",
      "NGOs : 0.06\n",
      "Eyewitness : 0.05\n",
      "Not applicable : 0.01\n",
      "\n",
      "\n",
      "Outsiders : 0.58\n",
      "Business : 0.10\n",
      "NGOs : 0.09\n",
      "Media : 0.08\n",
      "Government : 0.06\n",
      "Eyewitness : 0.06\n",
      "Not applicable : 0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Report: Strong quake off Guatemala is felt in Mexico City - http://t.co/fpjpvqES http://t.co/Y23hxGaF\",\n",
    "    \"thank you for all your concerns earthquake was scary but thankfully everyone praying for italy\",\n",
    "    \"we have water outside\",\n",
    "    \"help me! my house is destroyed i don't have roof\",\n",
    "    \"newspaper informs about earthquake in the ocean\",\n",
    "    \"cannot access my car\",\n",
    "    \"\"\n",
    "]\n",
    "model_clf = clf_ovr\n",
    "for text in texts:\n",
    "    print(text)\n",
    "    result = list(model_clf.predict_proba(np.array(get_doc_vector(tweet_parser_lemma(text))).reshape(1, -1)))[0]\n",
    "    zipped = zip(clf_ovr.classes_, result)\n",
    "    zipped = sorted(zipped, key=lambda x: x[1],reverse=True)\n",
    "    for i,(cl,prob) in enumerate(zipped):\n",
    "        print (\"{} : {:.2f}\".format(cl,prob))\n",
    "    print()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: Strong quake off Guatemala is felt in Mexico City - http://t.co/fpjpvqES http://t.co/Y23hxGaF\n",
      "Media Media 0.6673141376920387 \n",
      "\n",
      "RT @CalgaryPolice: Check here to see what areas of Montgomery must be evacuated: http://t.co/0p1PSfn001 #yycflood\n",
      "Government Government 0.6067768923190645 \n",
      "\n",
      "RT @willienelson: West has been in my backyard all my life.  My heart is praying for the community that we call home. #westtx\n",
      "Outsiders Outsiders 0.8088413575174989 \n",
      "\n",
      "@IdilioMon: #coloradoflood ==&gt; Street view from my home this afternoon. Rain, rain, go away... http://t.co/H3FdI3A1NQ\n",
      "Eyewitness Eyewitness 0.6075594921350587 \n",
      "\n",
      "GLOBE SUBS: Donate to Red Cross now. Text RED &lt;amt&gt; to 2899. Denoms. in 5, 25, 50, 100, 300, 500 and 1000. #YolandaPH #ReliefPH\n",
      "NGOs NGOs 0.7646046302386884 \n",
      "\n",
      "RT @joeshowradio: @joerogan The Bill of Rights was written for Dzhokar Tsarnaev: http://t.co/bmZUZRg4U4\n",
      "Not applicable Not applicable 0.6434451507343469 \n",
      "\n",
      "Hyundai, Mazda and Ford offering support to victims of Colorado flooding - Autoblog http://t.co/wPAIDzT2UV\n",
      "Business Business 0.6295445081241139 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tweets = [\n",
    "    ('Media', 'Report: Strong quake off Guatemala is felt in Mexico City - http://t.co/fpjpvqES http://t.co/Y23hxGaF'),\n",
    "    ('Government', 'RT @CalgaryPolice: Check here to see what areas of Montgomery must be evacuated: http://t.co/0p1PSfn001 #yycflood'),\n",
    "    ('Outsiders', 'RT @willienelson: West has been in my backyard all my life.  My heart is praying for the community that we call home. #westtx'),\n",
    "    ('Eyewitness', '@IdilioMon: #coloradoflood ==&gt; Street view from my home this afternoon. Rain, rain, go away... http://t.co/H3FdI3A1NQ'),\n",
    "    ('NGOs', 'GLOBE SUBS: Donate to Red Cross now. Text RED &lt;amt&gt; to 2899. Denoms. in 5, 25, 50, 100, 300, 500 and 1000. #YolandaPH #ReliefPH'),\n",
    "    ('Not applicable', 'RT @joeshowradio: @joerogan The Bill of Rights was written for Dzhokar Tsarnaev: http://t.co/bmZUZRg4U4'),\n",
    "    ('Business', 'Hyundai, Mazda and Ford offering support to victims of Colorado flooding - Autoblog http://t.co/wPAIDzT2UV')\n",
    "]\n",
    "\n",
    "model_clf = clf_ovr\n",
    "\n",
    "for (cl, text) in test_tweets:\n",
    "    #result = tp.classify_tweet(text)\n",
    "    result = list(model_clf.predict_proba(np.array(get_doc_vector(tweet_parser_lemma(text))).reshape(1, -1)))[0]\n",
    "    zipped = zip(clf_ovr.classes_, result)\n",
    "    zipped = sorted(zipped, key=lambda x: x[1],reverse=True)\n",
    "    result_class = zipped[0][0]\n",
    "    result_class_probability = zipped[0][1]\n",
    "    print(text)\n",
    "    print(cl, result_class , result_class_probability, \"\\n\")\n",
    "    assert cl == result_class\n",
    "    assert result_class_probability > 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kajo_semexp_core",
   "language": "python",
   "name": "kajo_semexp_core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
